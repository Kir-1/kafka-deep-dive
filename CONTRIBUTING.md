## Руководство по проведению экспериментов

### Шаг 1: Подготовка окружения

Для проведения экспериментов требуется работающий кластер Kafka и инструменты для взаимодействия с ним. Процесс подготовки включает следующие шаги.


1.  **Установите зависимости.**
    Для запуска Python-скриптов продюсера и потребителя требуется библиотека `kafka-python`.

    ```bash
    pip install -r requirements.txt
    ```

2.  **Запустите кластер Kafka.**
    Мы используем `docker-compose`, чтобы поднять кластер из трёх брокеров Kafka, Zookeeper и клиента с утилитами.

    ```bash
    docker-compose up -d
    ```

3.  **Создайте тестовый топик.**
    Создадим топик `my-test-topic` со следующими параметрами:
    *   `--partitions 4`: 4 партиции, чтобы продемонстрировать распараллеливание.
    *   `--replication-factor 3`: Каждая партиция будет скопирована на все 3 брокера для отказоустойчивости.

    ```bash
    docker-compose exec kafka-client kafka-topics.sh --bootstrap-server kafka1:9092 --create --topic my-test-topic --partitions 4 --replication-factor 3
    ```

4.  **Проверьте состояние топика.**
    Для проверки состояния топика и распределения партиций выполните команду:

    ```bash
    docker-compose exec kafka-client kafka-topics.sh --bootstrap-server kafka1:9092 --describe --topic my-test-topic
    ```

    Пример успешного вывода:
    ```
    Topic: my-test-topic      TopicId: <some_id>      PartitionCount: 4       ReplicationFactor: 3    Configs:
            Topic: my-test-topic      Partition: 0    Leader: 2       Replicas: 2,1,3      Isr: 2,1,3
            Topic: my-test-topic      Partition: 1    Leader: 3       Replicas: 3,2,1      Isr: 3,2,1
            Topic: my-test-topic      Partition: 2    Leader: 1       Replicas: 1,3,2      Isr: 1,3,2
            Topic: my-test-topic      Partition: 3    Leader: 2       Replicas: 2,3,1      Isr: 2,3,1
    ```
    > **Интерпретация вывода:**
    > *   **`PartitionCount: 4`**: Наш топик разделён на 4 части.
    > *   **`ReplicationFactor: 3`**: Каждая партиция имеет 3 копии.
    > *   **`Leader`**: Брокер, отвечающий за чтение и запись в данную партицию (например, брокер `2` для партиции `0`).
    > *   **`Replicas`**: Список всех брокеров, хранящих копию партиции.
    > *   **`Isr` (In-Sync Replicas)**: Список реплик, которые не отстают от лидера и готовы стать новым лидером в случае сбоя.

### Эксперимент 1: Consumer Groups и распараллеливание

Этот эксперимент наглядно демонстрирует, как партиции позволяют масштабировать обработку сообщений. Сначала мы наполним топик данными.

**Запустите продюсера:**
```bash
python producer.py
```
Он отправит 20 сообщений с 4 разными ключами. Сообщения с одинаковым ключом попадут в одну и ту же партицию.

---

#### Сценарий A: Один потребитель

Запустим одного потребителя в группе `group-1`.

*   **Действие:** Откройте терминал и выполните:
    ```bash
    python consumer.py group-1
    ```
*   **Результат:** Вы увидите, что этот потребитель получает сообщения из **всех четырёх партиций** (`Partition: 0`, `Partition: 1`, `Partition: 2`, `Partition: 3`).
*   **Вывод:** ✅ Когда потребитель в группе один, Kafka назначает ему все доступные партиции топика.

---

#### Сценарий B: Распараллеливание нагрузки

Теперь посмотрим, как Kafka распределит работу между несколькими потребителями в одной группе.

*   **Действие:** Остановите предыдущего потребителя (`Ctrl+C`). Откройте **четыре** терминала и в каждом запустите потребителя в **одной и той же группе** `group-2`.

    ```bash
    # Терминал 1
    python consumer.py group-2

    # Терминал 2
    python consumer.py group-2
    
    # ... и так далее для 3 и 4
    ```
*   **Результат:** Каждый из четырёх терминалов начнёт получать сообщения только из **своей уникальной партиции**. Например, первый терминал будет видеть только `Partition: 0`, второй — только `Partition: 2` и т.д.
*   **Вывод:** ✅ Kafka распределила партиции между потребителями в группе. Это и есть **горизонтальное масштабирование** обработки: каждый экземпляр приложения работает параллельно над своим подмножеством данных.

---

#### Сценарий C: Лишний потребитель

Что будет, если потребителей станет больше, чем партиций?

*   **Действие:** Не закрывая предыдущие четыре терминала, откройте **пятый** и запустите в нём ещё одного потребителя из той же группы `group-2`.

    ```bash
    # Терминал 5
    python consumer.py group-2
    ```
*   **Результат:** Этот пятый потребитель запустится, но **не получит ни одного сообщения**. Он будет простаивать.
*   **Вывод:** ✅ Количество активных потребителей в группе ограничено количеством партиций в топике. Все партиции уже заняты, поэтому пятому потребителю не досталось работы. Он ждёт, пока одна из партиций освободится (например, если один из других потребителей упадёт).

---

### Эксперимент 2: Отказоустойчивость и выборы лидера

Проверим, как Kafka справляется со сбоями брокеров, автоматически переключая лидера на одну из реплик.

1.  **Определим цель:** Посмотрим ещё раз на состояние топика.
    ```bash
    docker-compose exec kafka-client kafka-topics.sh --bootstrap-server kafka1:9092 --describe --topic my-test-topic
    ```
    Допустим, для **Партиции 2** лидером является брокер **1** (`Leader: 1`). Наша цель — вывести его из строя.

2.  **Имитируем сбой:** Остановим контейнер с брокером `kafka1`.
    ```bash
    docker-compose stop kafka1
    ```

3.  **Проверяем результат:** Подождите 5-10 секунд и запросите состояние топика снова, но уже через работающий брокер (`kafka2:9093`).

    ```bash
    docker-compose exec kafka-client kafka-topics.sh \
      --bootstrap-server kafka2:9093 \
      --describe \
      --topic my-test-topic
    ```
    Вы увидите, что картина для Партиции 2 изменилась:
    ```
    ...
    Topic: my-test-topic      Partition: 2    Leader: 3       Replicas: 1,3,2      Isr: 3,2
    ...
    ```
*   **Вывод:** ✅ Kafka обнаружила сбой и провела **выборы нового лидера**. Им стал один из брокеров из списка `Isr` (например, брокер `3`). Брокер `1` всё ещё числится в `Replicas`, но отсутствует в `Isr`, так как он не в сети. Кластер продолжает работать, а потребители могут читать данные без перебоев.

4.  **Восстановление:** Вернём брокер в строй.
    ```bash
    docker-compose start kafka1
    ```
    Через некоторое время он синхронизирует свои данные и вернётся в список `Isr` для своих партиций.

---

### Эксперимент 3: Хранение смещений (Offsets)

Где Kafka хранит "закладки" для каждой consumer group? Ответ — в специальном внутреннем топике `__consumer_offsets`. Давайте заглянем в него.

1.  **Прочитаем топик `__consumer_offsets`:**
    Данные в этом топике хранятся в бинарном формате, поэтому для их чтения мы используем специальный форматер. Команда покажет последние закоммиченные смещения для всех групп.

    ```bash
    docker-compose exec kafka-client kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic __consumer_offsets --formatter 'kafka.coordinator.group.GroupMetadataManager$OffsetsMessageFormatter' --from-beginning --timeout-ms 5000
    ```
2.  **Анализируем результат:**
    Среди служебной информации вы найдете строки, относящиеся к нашей `group-2`:
    ```
    [group-2,my-test-topic,0]::[OffsetMetadata[offset=5,leaderEpoch=...],CommitTime=...,ExpirationTime=...]
    [group-2,my-test-topic,1]::[OffsetMetadata[offset=5,leaderEpoch=...],CommitTime=...,ExpirationTime=...]
    [group-2,my-test-topic,2]::[OffsetMetadata[offset=5,leaderEpoch=...],CommitTime=...,ExpirationTime=...]
    [group-2,my-test-topic,3]::[OffsetMetadata[offset=5,leaderEpoch=...],CommitTime=...,ExpirationTime=...]
    ```
*   **Вывод:** ✅ Kafka хранит смещение для каждой пары `(consumer_group, partition)`. Ключ сообщения — это `[group-2,my-test-topic,0]`, а значение (`offset=5`) — это номер следующего сообщения, которое будет прочитано. Такой подход делает хранение смещений таким же надёжным и масштабируемым, как и хранение обычных данных.

---

### Эксперимент 4: Физическое хранение данных на диске

Наконец, посмотрим, как партиции выглядят на файловой системе самого брокера.

1.  **Зайдём внутрь контейнера `kafka1`:**
    ```bash
    docker-compose exec kafka1 /bin/bash
    ```

2.  **Перейдём в директорию с данными:**
    В образе от Bitnami данные хранятся в `/bitnami/kafka/data`.
    ```bash
    cd /bitnami/kafka/data
    ```

3.  **Осмотримся:**
    Вы увидите папки, названные по шаблону `имя_топика-номер_партиции`.
    ```
    $ ls -l
    ...
    drwxr-sr-x 2 1001 root 4096 Nov 26 18:30 __consumer_offsets-12
    drwxr-sr-x 2 1001 root 4096 Nov 26 18:31 my-test-topic-0
    drwxr-sr-x 2 1001 root 4096 Nov 26 18:31 my-test-topic-2
    ...
    ```
    > Брокер `kafka1` хранит данные только тех партиций, для которых он является либо лидером, либо репликой (в нашем случае это `0` и `2`).

4.  **Заглянем в папку партиции:**
    ```bash
    cd my-test-topic-2/
    ls -l
    ```
    Внутри вы увидите файлы-сегменты:
    ```
    -rw-r--r-- 1 1001 root 10485760 Nov 26 18:31 00000000000000000000.index
    -rw-r--r-- 1 1001 root      165 Nov 26 18:31 00000000000000000000.log
    -rw-r--r-- 1 1001 root 10485760 Nov 26 18:31 00000000000000000000.timeindex
    -rw-r--r-- 1 1001 root       10 Nov 26 18:31 leader-epoch-checkpoint
    ```
*   **Вывод:** ✅ Каждая партиция на диске — это набор файлов-сегментов. `.log` содержит сами сообщения, а `.index` и `.timeindex` служат для быстрой навигации по логу по смещению или времени. Это и есть низкоуровневое представление данных в Kafka.

### Завершение

После всех экспериментов не забудьте остановить и удалить контейнеры, чтобы освободить ресурсы.

```bash
docker-compose down
```